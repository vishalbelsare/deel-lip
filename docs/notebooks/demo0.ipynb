{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 0: Example and usage\n",
    "\n",
    "In order to make things simple the following rules have been followed\n",
    "during development:\n",
    "\n",
    "-   `deel-lip` follows the `keras` package structure.\n",
    "-   All elements (layers, activations, initializers, ...) are compatible\n",
    "    with standard the `keras` elements.\n",
    "-   When a k-Lipschitz layer overrides a standard keras layer, it uses\n",
    "    the same interface and the same parameters. The only difference is a\n",
    "    new parameter to control the Lipschitz constant of a layer.\n",
    "\n",
    "## Which layers are safe to use?\n",
    "\n",
    "The following table indicates which layers are safe to use in a Lipshitz\n",
    "network, and which are not.\n",
    "\n",
    "| layer                                                                                         | 1-lip? | deel-lip equivalent                                                                                         | comments                                                                          |\n",
    "|-----------------------------------------------------------------------------------------------|--------|-------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------|\n",
    "| `Dense`                                                                                       | no     | `SpectralDense`<br>`FrobeniusDense`                       | `SpectralDense` and `FrobeniusDense` are similar when there is a single output. |\n",
    "| `Conv2D`                                                                                      | no     | `SpectralConv2D`<br>`FrobeniusConv2D`                     | `SpectralConv2D` also implements Bj√∂rck normalization.                           |\n",
    "| `MaxPooling`<br>`GlobalMaxPooling`             | yes    | n/a                                                                                                         |                                                                                   |\n",
    "| `AveragePooling2D`<br>`GlobalAveragePooling2D` | no     | `ScaledAveragePooling2D`<br>`ScaledGlobalAveragePooling2D` | The lipschitz constant is bounded by `sqrt(pool_h * pool_h)`.                     |\n",
    "| `Flatten`                                                                                     | yes    | n/a                                                                                                         |                                                                                   |\n",
    "| `Dropout`                                                                                     | no     | None                                                                                                        | The lipschitz constant is bounded by the dropout factor.                          |\n",
    "| `BatchNormalization`                                                                          | no     | None                                                                                                        | We suspect that layer normalization already limits internal covariate shift.      |\n",
    "\n",
    "## Design tips\n",
    "\n",
    "Designing lipschitz networks requires a careful design in order to avoid\n",
    "vanishing/exploding gradient problems.\n",
    "\n",
    "Choosing pooling layers:\n",
    "\n",
    "| layer                                                                            | advantages                                                                   | disadvantages                                                                      |\n",
    "|----------------------------------------------------------------------------------|------------------------------------------------------------------------------|------------------------------------------------------------------------------------|\n",
    "| `ScaledAveragePooling2D` and `MaxPooling2D`                                      | very similar to original implementation (just add a scaling factor for avg). | not norm preserving nor gradient norm preserving.                                  |\n",
    "| `InvertibleDownSampling`                                                         | norm preserving and gradient norm preserving.                                | increases the number of channels (and the number of parameters of the next layer). |\n",
    "| `ScaledL2NormPooling2D` (_sqrt(avgpool(x\\*\\*2))_) | norm preserving.                                                             | lower numerical stability of the gradient when inputs are close to zero.           |\n",
    "\n",
    "Choosing activations:\n",
    "\n",
    "| layer                                                                  | advantages                                                                                   | disadvantages                                                                                  |\n",
    "|------------------------------------------------------------------------|----------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------|\n",
    "| `ReLU`                                                                 |                                                                                              | create a strong vanishing gradient effect. If you manage to learn with it, please call 911.    |\n",
    "| `MaxMin` (_stack(\\[ReLU(x), ReLU(-x)\\])_) | have similar properties to ReLU, but is norm and gradient norm preserving                    | double the number of outputs                                                                   |\n",
    "| `GroupSort`                                                            | Input and GradientNorm preserving. Also limit the need of biases (as it is shift invariant). | more computationally expensive, (when its parameter _n_ is large) |\n",
    "\n",
    "Please note that when learning with the `HKR_loss` and `HKR_multiclass_loss`, no\n",
    "activation is required on the last layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use it ?\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/deel-ai/deel-lip/blob/master/docs/notebooks/demo0.ipynb)\n",
    "\n",
    "Here is an example of 1-lipschitz network trained on MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thibaut.boissin/projects/deel-lip/deel/lip/model.py:56: UserWarning: Sequential model contains a layer wich is not a Lipschitz layer: flatten_2\n",
      "  layer.name\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"hkr_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "spectral_conv2d_4 (SpectralC (None, 28, 28, 16)        321       \n",
      "_________________________________________________________________\n",
      "scaled_l2norm_pooling2d_4 (S (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "spectral_conv2d_5 (SpectralC (None, 14, 14, 16)        4641      \n",
      "_________________________________________________________________\n",
      "scaled_l2norm_pooling2d_5 (S (None, 7, 7, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "spectral_dense_2 (SpectralDe (None, 32)                50241     \n",
      "_________________________________________________________________\n",
      "frobenius_dense_2 (Frobenius (None, 10)                640       \n",
      "=================================================================\n",
      "Total params: 55,843\n",
      "Trainable params: 27,920\n",
      "Non-trainable params: 27,923\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "30/30 [==============================] - 3s 43ms/step - loss: 6.5323 - accuracy: 0.1522 - MulticlassKR: 0.0183 - val_loss: 2.3933 - val_accuracy: 0.4873 - val_MulticlassKR: 0.0942\n",
      "Epoch 2/30\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 2.0856 - accuracy: 0.5528 - MulticlassKR: 0.1149 - val_loss: 1.3480 - val_accuracy: 0.7091 - val_MulticlassKR: 0.1653\n",
      "Epoch 3/30\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 1.2743 - accuracy: 0.7298 - MulticlassKR: 0.1743 - val_loss: 0.9228 - val_accuracy: 0.7942 - val_MulticlassKR: 0.2097\n",
      "Epoch 4/30\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 0.9001 - accuracy: 0.7975 - MulticlassKR: 0.2168 - val_loss: 0.6864 - val_accuracy: 0.8368 - val_MulticlassKR: 0.2486\n",
      "Epoch 5/30\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.6889 - accuracy: 0.8338 - MulticlassKR: 0.2546 - val_loss: 0.5352 - val_accuracy: 0.8650 - val_MulticlassKR: 0.2835\n",
      "Epoch 6/30\n",
      "30/30 [==============================] - 1s 37ms/step - loss: 0.5256 - accuracy: 0.8609 - MulticlassKR: 0.2879 - val_loss: 0.4442 - val_accuracy: 0.8805 - val_MulticlassKR: 0.3166\n",
      "Epoch 7/30\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.4469 - accuracy: 0.8735 - MulticlassKR: 0.3186 - val_loss: 0.3349 - val_accuracy: 0.8911 - val_MulticlassKR: 0.3470\n",
      "Epoch 8/30\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.3493 - accuracy: 0.8835 - MulticlassKR: 0.3480 - val_loss: 0.2641 - val_accuracy: 0.8961 - val_MulticlassKR: 0.3787\n",
      "Epoch 9/30\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.2722 - accuracy: 0.8938 - MulticlassKR: 0.3818 - val_loss: 0.2122 - val_accuracy: 0.8993 - val_MulticlassKR: 0.4127\n",
      "Epoch 10/30\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.2036 - accuracy: 0.9013 - MulticlassKR: 0.4153 - val_loss: 0.1330 - val_accuracy: 0.9079 - val_MulticlassKR: 0.4487\n",
      "Epoch 11/30\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.1472 - accuracy: 0.9059 - MulticlassKR: 0.4505 - val_loss: 0.0799 - val_accuracy: 0.9126 - val_MulticlassKR: 0.4861\n",
      "Epoch 12/30\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0939 - accuracy: 0.9103 - MulticlassKR: 0.4915 - val_loss: 0.0371 - val_accuracy: 0.9142 - val_MulticlassKR: 0.5313\n",
      "Epoch 13/30\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.0499 - accuracy: 0.9100 - MulticlassKR: 0.5346 - val_loss: -0.0211 - val_accuracy: 0.9206 - val_MulticlassKR: 0.5729\n",
      "Epoch 14/30\n",
      "30/30 [==============================] - 1s 39ms/step - loss: -0.0216 - accuracy: 0.9162 - MulticlassKR: 0.5760 - val_loss: -0.0682 - val_accuracy: 0.9200 - val_MulticlassKR: 0.6219\n",
      "Epoch 15/30\n",
      "30/30 [==============================] - 1s 35ms/step - loss: -0.0666 - accuracy: 0.9168 - MulticlassKR: 0.6248 - val_loss: -0.1301 - val_accuracy: 0.9236 - val_MulticlassKR: 0.6742\n",
      "Epoch 16/30\n",
      "30/30 [==============================] - 1s 35ms/step - loss: -0.1223 - accuracy: 0.9197 - MulticlassKR: 0.6778 - val_loss: -0.1777 - val_accuracy: 0.9270 - val_MulticlassKR: 0.7275\n",
      "Epoch 17/30\n",
      "30/30 [==============================] - 1s 36ms/step - loss: -0.1605 - accuracy: 0.9199 - MulticlassKR: 0.7291 - val_loss: -0.2426 - val_accuracy: 0.9272 - val_MulticlassKR: 0.7900\n",
      "Epoch 18/30\n",
      "30/30 [==============================] - 1s 36ms/step - loss: -0.2278 - accuracy: 0.9218 - MulticlassKR: 0.7886 - val_loss: -0.2883 - val_accuracy: 0.9305 - val_MulticlassKR: 0.8471\n",
      "Epoch 19/30\n",
      "30/30 [==============================] - 1s 40ms/step - loss: -0.2246 - accuracy: 0.9170 - MulticlassKR: 0.8478 - val_loss: -0.3104 - val_accuracy: 0.9183 - val_MulticlassKR: 0.9070\n",
      "Epoch 20/30\n",
      "30/30 [==============================] - 1s 34ms/step - loss: -0.3066 - accuracy: 0.9213 - MulticlassKR: 0.9085 - val_loss: -0.3778 - val_accuracy: 0.9284 - val_MulticlassKR: 0.9754\n",
      "Epoch 21/30\n",
      "30/30 [==============================] - 1s 39ms/step - loss: -0.3736 - accuracy: 0.9241 - MulticlassKR: 0.9739 - val_loss: -0.4258 - val_accuracy: 0.9280 - val_MulticlassKR: 1.0388\n",
      "Epoch 22/30\n",
      "30/30 [==============================] - 1s 35ms/step - loss: -0.4180 - accuracy: 0.9229 - MulticlassKR: 1.0337 - val_loss: -0.4805 - val_accuracy: 0.9302 - val_MulticlassKR: 1.1069\n",
      "Epoch 23/30\n",
      "30/30 [==============================] - 1s 38ms/step - loss: -0.4624 - accuracy: 0.9234 - MulticlassKR: 1.1055 - val_loss: -0.5607 - val_accuracy: 0.9312 - val_MulticlassKR: 1.1803\n",
      "Epoch 24/30\n",
      "30/30 [==============================] - 1s 35ms/step - loss: -0.5279 - accuracy: 0.9257 - MulticlassKR: 1.1797 - val_loss: -0.5866 - val_accuracy: 0.9275 - val_MulticlassKR: 1.2456\n",
      "Epoch 25/30\n",
      "30/30 [==============================] - 1s 38ms/step - loss: -0.5482 - accuracy: 0.9218 - MulticlassKR: 1.2388 - val_loss: -0.6441 - val_accuracy: 0.9310 - val_MulticlassKR: 1.3125\n",
      "Epoch 26/30\n",
      "30/30 [==============================] - 1s 35ms/step - loss: -0.6375 - accuracy: 0.9263 - MulticlassKR: 1.3103 - val_loss: -0.6890 - val_accuracy: 0.9295 - val_MulticlassKR: 1.3795\n",
      "Epoch 27/30\n",
      "30/30 [==============================] - 1s 42ms/step - loss: -0.6668 - accuracy: 0.9230 - MulticlassKR: 1.3719 - val_loss: -0.7413 - val_accuracy: 0.9271 - val_MulticlassKR: 1.4496\n",
      "Epoch 28/30\n",
      "30/30 [==============================] - 1s 35ms/step - loss: -0.7483 - accuracy: 0.9264 - MulticlassKR: 1.4371 - val_loss: -0.7748 - val_accuracy: 0.9296 - val_MulticlassKR: 1.5096\n",
      "Epoch 29/30\n",
      "30/30 [==============================] - 1s 49ms/step - loss: -0.7495 - accuracy: 0.9229 - MulticlassKR: 1.4900 - val_loss: -0.8622 - val_accuracy: 0.9332 - val_MulticlassKR: 1.5644\n",
      "Epoch 30/30\n",
      "30/30 [==============================] - 1s 35ms/step - loss: -0.8047 - accuracy: 0.9246 - MulticlassKR: 1.5530 - val_loss: -0.8732 - val_accuracy: 0.9297 - val_MulticlassKR: 1.6220\n"
     ]
    }
   ],
   "source": [
    "from deel.lip.layers import (\n",
    "    SpectralDense,\n",
    "    SpectralConv2D,\n",
    "    ScaledL2NormPooling2D,\n",
    "    FrobeniusDense,\n",
    ")\n",
    "from deel.lip.model import Sequential\n",
    "from deel.lip.activations import GroupSort\n",
    "from deel.lip.losses import MulticlassHKR, MulticlassKR\n",
    "from tensorflow.keras.layers import Input, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "# Sequential (resp Model) from deel.model has the same properties as any lipschitz model.\n",
    "# It act only as a container, with features specific to lipschitz\n",
    "# functions (condensation, vanilla_exportation...) but The layers are fully compatible\n",
    "# with the tf.keras.model.Sequential/Model\n",
    "model = Sequential(\n",
    "    [\n",
    "        Input(shape=(28, 28, 1)),\n",
    "        # Lipschitz layers preserve the API of their superclass ( here Conv2D )\n",
    "        # an optional param is available: k_coef_lip which control the lipschitz\n",
    "        # constant of the layer\n",
    "        SpectralConv2D(\n",
    "            filters=16,\n",
    "            kernel_size=(3, 3),\n",
    "            activation=GroupSort(2),\n",
    "            use_bias=True,\n",
    "            kernel_initializer=\"orthogonal\",\n",
    "        ),\n",
    "        # usual pooling layer are implemented (avg, max...), but new layers are also available\n",
    "        ScaledL2NormPooling2D(pool_size=(2, 2), data_format=\"channels_last\"),\n",
    "        SpectralConv2D(\n",
    "            filters=16,\n",
    "            kernel_size=(3, 3),\n",
    "            activation=GroupSort(2),\n",
    "            use_bias=True,\n",
    "            kernel_initializer=\"orthogonal\",\n",
    "        ),\n",
    "        ScaledL2NormPooling2D(pool_size=(2, 2), data_format=\"channels_last\"),\n",
    "        # our layers are fully interoperable with existing keras layers\n",
    "        Flatten(),\n",
    "        SpectralDense(\n",
    "            32,\n",
    "            activation=GroupSort(2),\n",
    "            use_bias=True,\n",
    "            kernel_initializer=\"orthogonal\",\n",
    "        ),\n",
    "        FrobeniusDense(\n",
    "            10, activation=None, use_bias=False, kernel_initializer=\"orthogonal\"\n",
    "        ),\n",
    "    ],\n",
    "    # similary model has a parameter to set the lipschitz constant\n",
    "    # to set automatically the constant of each layer\n",
    "    k_coef_lip=1.0,\n",
    "    name=\"hkr_model\",\n",
    ")\n",
    "\n",
    "# HKR (Hinge-Krantorovich-Rubinstein) optimize robustness along with accuracy\n",
    "model.compile(\n",
    "    # decreasing alpha and increasing min_margin improve robustness (at the cost of accuracy)\n",
    "    # note also in the case of lipschitz networks, more robustness require more parameters.\n",
    "    loss=MulticlassHKR(alpha=50, min_margin=0.05),\n",
    "    optimizer=Adam(1e-3),\n",
    "    metrics=[\"accuracy\", MulticlassKR()],\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# load data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# standardize and reshape the data\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "mean = x_train.mean()\n",
    "std = x_train.std()\n",
    "x_train = (x_train - mean) / std\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "x_test = (x_test - mean) / std\n",
    "# one hot encode the labels\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# fit the model\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=2048,\n",
    "    epochs=30,\n",
    "    validation_data=(x_test, y_test),\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# once training is finished you can convert\n",
    "# SpectralDense layers into Dense layers and SpectralConv2D into Conv2D\n",
    "# which optimize performance for inference\n",
    "vanilla_model = model.vanilla_export()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e585d72a124540032141457729caea4129d351be49f1f69f41c00c4f8476abb5"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('tf24': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
