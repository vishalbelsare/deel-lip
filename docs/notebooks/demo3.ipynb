{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Demo 3: HKR classifier on MNIST dataset\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/deel-ai/deel-lip/blob/master/docs/notebooks/demo3.ipynb)\n",
    "\n",
    "This notebook will demonstrate learning a binary task on the MNIST0-8 dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# pip install deel-lip -qqq"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.python.keras.layers import Input, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import binary_accuracy\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from deel.lip.layers import (\n",
    "    SpectralConv2D,\n",
    "    SpectralDense,\n",
    "    FrobeniusDense,\n",
    "    ScaledL2NormPooling2D,\n",
    ")\n",
    "from deel.lip.activations import MaxMin, GroupSort, GroupSort2, FullSort\n",
    "from deel.lip.losses import HKR, KR, HingeMargin"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-08 18:34:34.803681: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### data preparation\n",
    "\n",
    "For this task we will select two classes: 0 and 8. Labels are changed to {-1,1}, wich is compatible\n",
    "with the Hinge term used in the loss."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# first we select the two classes\n",
    "selected_classes = [0, 8]  # must be two classes as we perform binary classification\n",
    "\n",
    "\n",
    "def prepare_data(x, y, class_a=0, class_b=8):\n",
    "    \"\"\"\n",
    "    This function convert the MNIST data to make it suitable for our binary classification\n",
    "    setup.\n",
    "    \"\"\"\n",
    "    # select items from the two selected classes\n",
    "    mask = (y == class_a) + (\n",
    "        y == class_b\n",
    "    )  # mask to select only items from class_a or class_b\n",
    "    x = x[mask]\n",
    "    y = y[mask]\n",
    "    x = x.astype(\"float32\")\n",
    "    y = y.astype(\"float32\")\n",
    "    # convert from range int[0,255] to float32[-1,1]\n",
    "    x /= 255\n",
    "    x = x.reshape((-1, 28, 28, 1))\n",
    "    # change label to binary classification {-1,1}\n",
    "    y[y == class_a] = 1.0\n",
    "    y[y == class_b] = -1.0\n",
    "    return x, y\n",
    "\n",
    "\n",
    "# now we load the dataset\n",
    "(x_train, y_train_ord), (x_test, y_test_ord) = mnist.load_data()\n",
    "\n",
    "# prepare the data\n",
    "x_train, y_train = prepare_data(\n",
    "    x_train, y_train_ord, selected_classes[0], selected_classes[1]\n",
    ")\n",
    "x_test, y_test = prepare_data(\n",
    "    x_test, y_test_ord, selected_classes[0], selected_classes[1]\n",
    ")\n",
    "\n",
    "# display infos about dataset\n",
    "print(\n",
    "    \"train set size: %i samples, classes proportions: %.3f percent\"\n",
    "    % (y_train.shape[0], 100 * y_train[y_train == 1].sum() / y_train.shape[0])\n",
    ")\n",
    "print(\n",
    "    \"test set size: %i samples, classes proportions: %.3f percent\"\n",
    "    % (y_test.shape[0], 100 * y_test[y_test == 1].sum() / y_test.shape[0])\n",
    ")\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train set size: 11774 samples, classes proportions: 50.306 percent\n",
      "test set size: 1954 samples, classes proportions: 50.154 percent\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Build lipschitz Model\n",
    "\n",
    "Let's first explicit the paremeters of this experiment"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# training parameters\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "# network parameters\n",
    "activation = GroupSort  # ReLU, MaxMin, GroupSort2\n",
    "\n",
    "# loss parameters\n",
    "min_margin = 1.0\n",
    "alpha = 10.0\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can build the network.\n",
    "Here the experiment is done with a MLP. But `Deel-lip` also provide state of the art 1-Lipschitz convolutions."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "K.clear_session()\n",
    "# helper function to build the 1-lipschitz MLP\n",
    "wass = Sequential(\n",
    "    layers=[\n",
    "        Input((28, 28, 1)),\n",
    "        Flatten(),\n",
    "        SpectralDense(32, GroupSort2(), use_bias=True),\n",
    "        SpectralDense(16, GroupSort2(), use_bias=True),\n",
    "        FrobeniusDense(1, activation=None, use_bias=False),\n",
    "    ],\n",
    "    name=\"lipModel\",\n",
    ")\n",
    "wass.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"lipModel\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "spectral_dense (SpectralDens (None, 32)                50241     \n",
      "_________________________________________________________________\n",
      "spectral_dense_1 (SpectralDe (None, 16)                1057      \n",
      "_________________________________________________________________\n",
      "frobenius_dense (FrobeniusDe (None, 1)                 32        \n",
      "=================================================================\n",
      "Total params: 51,330\n",
      "Trainable params: 25,664\n",
      "Non-trainable params: 25,666\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "optimizer = Adam(lr=0.001)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# as the output of our classifier is in the real range [-1, 1], binary accuracy must be redefined\n",
    "def HKR_binary_accuracy(y_true, y_pred):\n",
    "    S_true = tf.dtypes.cast(tf.greater_equal(y_true[:, 0], 0), dtype=tf.float32)\n",
    "    S_pred = tf.dtypes.cast(tf.greater_equal(y_pred[:, 0], 0), dtype=tf.float32)\n",
    "    return binary_accuracy(S_true, S_pred)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "wass.compile(\n",
    "    loss=HKR(\n",
    "        alpha=alpha, min_margin=min_margin\n",
    "    ),  # HKR stands for the hinge regularized KR loss\n",
    "    metrics=[\n",
    "        KR,  # shows the KR term of the loss\n",
    "        HingeMargin(min_margin=min_margin),  # shows the hinge term of the loss\n",
    "        HKR_binary_accuracy,  # shows the classification accuracy\n",
    "    ],\n",
    "    optimizer=optimizer,\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Learn classification on MNIST\n",
    "\n",
    "Now the model is build, we can learn the task."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "wass.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    validation_data=(x_test, y_test),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    epochs=epochs,\n",
    "    verbose=1,\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "92/92 [==============================] - 2s 10ms/step - loss: -1.6675 - KR: 3.7144 - HingeMargin: 0.2047 - HKR_binary_accuracy: 0.9382 - val_loss: -5.0961 - val_KR: 5.5990 - val_HingeMargin: 0.0519 - val_HKR_binary_accuracy: 0.9786\n",
      "Epoch 2/10\n",
      "92/92 [==============================] - 1s 7ms/step - loss: -5.0297 - KR: 5.5716 - HingeMargin: 0.0542 - HKR_binary_accuracy: 0.9793 - val_loss: -5.4469 - val_KR: 5.7710 - val_HingeMargin: 0.0354 - val_HKR_binary_accuracy: 0.9879\n",
      "Epoch 3/10\n",
      "92/92 [==============================] - 1s 7ms/step - loss: -5.3788 - KR: 5.7838 - HingeMargin: 0.0405 - HKR_binary_accuracy: 0.9858 - val_loss: -5.6435 - val_KR: 5.9555 - val_HingeMargin: 0.0334 - val_HKR_binary_accuracy: 0.9860\n",
      "Epoch 4/10\n",
      "92/92 [==============================] - 1s 8ms/step - loss: -5.6172 - KR: 5.9671 - HingeMargin: 0.0350 - HKR_binary_accuracy: 0.9874 - val_loss: -5.7918 - val_KR: 6.0764 - val_HingeMargin: 0.0308 - val_HKR_binary_accuracy: 0.9879\n",
      "Epoch 5/10\n",
      "92/92 [==============================] - 1s 7ms/step - loss: -5.7598 - KR: 6.0676 - HingeMargin: 0.0308 - HKR_binary_accuracy: 0.9891 - val_loss: -5.8711 - val_KR: 6.1062 - val_HingeMargin: 0.0264 - val_HKR_binary_accuracy: 0.9899\n",
      "Epoch 6/10\n",
      "92/92 [==============================] - 1s 7ms/step - loss: -5.7647 - KR: 6.0829 - HingeMargin: 0.0318 - HKR_binary_accuracy: 0.9879 - val_loss: -5.8503 - val_KR: 6.1463 - val_HingeMargin: 0.0315 - val_HKR_binary_accuracy: 0.9879\n",
      "Epoch 7/10\n",
      "92/92 [==============================] - 1s 7ms/step - loss: -5.8007 - KR: 6.1082 - HingeMargin: 0.0307 - HKR_binary_accuracy: 0.9884 - val_loss: -5.8470 - val_KR: 6.1179 - val_HingeMargin: 0.0296 - val_HKR_binary_accuracy: 0.9879\n",
      "Epoch 8/10\n",
      "92/92 [==============================] - 1s 7ms/step - loss: -5.8268 - KR: 6.1185 - HingeMargin: 0.0292 - HKR_binary_accuracy: 0.9897 - val_loss: -5.8439 - val_KR: 6.1153 - val_HingeMargin: 0.0294 - val_HKR_binary_accuracy: 0.9889\n",
      "Epoch 9/10\n",
      "92/92 [==============================] - 1s 7ms/step - loss: -5.8865 - KR: 6.1548 - HingeMargin: 0.0268 - HKR_binary_accuracy: 0.9910 - val_loss: -5.8800 - val_KR: 6.1668 - val_HingeMargin: 0.0312 - val_HKR_binary_accuracy: 0.9874\n",
      "Epoch 10/10\n",
      "92/92 [==============================] - 1s 7ms/step - loss: -5.8578 - KR: 6.1453 - HingeMargin: 0.0288 - HKR_binary_accuracy: 0.9892 - val_loss: -5.9233 - val_KR: 6.1783 - val_HingeMargin: 0.0282 - val_HKR_binary_accuracy: 0.9889\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fce2c6635d0>"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see the model reach a very decent accuracy on this task."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('tf24': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "interpreter": {
   "hash": "e585d72a124540032141457729caea4129d351be49f1f69f41c00c4f8476abb5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
